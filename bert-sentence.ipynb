{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599764857751",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with a pretrained model\n",
    "model_name = 'bert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "granola_ids [3, 1671, 4058, 39, 14156, 23, 4]\ngranola_tokens ['[CLS]', 'Ich', 'bin', 'ein', 'Mini', '##on', '[SEP]']\n"
    }
   ],
   "source": [
    "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
    "granola_ids = tokenizer.encode('Ich bin ein Minion')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "# Convert the IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "granola_ids tensor([    3,  1671,  4058,    39, 14156,    23,     4])\n"
    }
   ],
   "source": [
    "# Convert the list of IDs to a tensor of IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "granola_ids = granola_ids.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([7])\ntorch.Size([1, 7])\n<class 'torch.Tensor'>\n<class 'tuple'>\n3\n13\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dff6ae06f043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(granola_ids.size())\n",
    "# unsqueeze IDs to get batch size of 1 as added dimension\n",
    "granola_ids = granola_ids.unsqueeze(0)\n",
    "print(granola_ids.size())\n",
    "\n",
    "print(type(granola_ids))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=granola_ids)\n",
    "\n",
    "# the output is a tuple\n",
    "print(type(out))\n",
    "# the tuple contains three elements as explained above)\n",
    "print(len(out))\n",
    "# we only want the hidden_states\n",
    "hidden_states = out[2]\n",
    "print(len(hidden_states))\n",
    "print(hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30000, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-1.7320e-01,  4.0907e-02,  4.5438e-01, -4.2997e-02,  1.8002e-01,\n         5.4788e-01, -4.8228e-02, -4.2350e-01, -8.4088e-01,  7.3957e-01,\n        -3.6593e-01,  3.5973e-01,  8.3447e-02, -3.2965e-01,  8.1233e-01,\n         8.9030e-02,  5.6575e-01, -5.2960e-01,  4.3412e-01,  6.6688e-02,\n        -8.7260e-02,  3.8725e-01, -4.9636e-01, -3.0553e-01, -9.4124e-01,\n         5.3892e-01,  4.1416e-02, -3.7828e-01, -4.5504e-01,  2.9940e-01,\n         7.2828e-04,  2.3506e-02, -3.7628e-02, -3.0512e-01, -8.8771e-03,\n        -4.5097e-01, -3.8860e-01, -7.2443e-01,  5.2398e-01,  1.8769e-02,\n        -5.7824e-02,  7.2235e-01, -7.1317e-01, -1.7501e-01,  1.5490e-01,\n         4.5173e-02, -3.7515e-01, -1.8814e-01, -6.6256e-01, -1.8524e-02,\n        -3.7364e-01, -4.2808e-01,  7.0594e-01, -1.9603e-01,  3.3559e-02,\n         1.5797e-01, -4.0003e-02,  5.2094e-01, -5.2013e-01,  1.0825e-01,\n         3.2536e-01,  7.7148e-01, -1.2824e-01, -1.2871e-01,  2.3662e-01,\n        -3.7407e-01, -1.7652e-02, -2.4922e-01, -2.2622e-01, -5.0114e-01,\n        -2.0157e-01,  2.9616e-02, -3.0162e+00,  5.0735e-01, -3.3961e-03,\n         2.9862e-01,  6.3384e-02,  1.5001e-01, -5.2316e-01,  4.9287e-01,\n        -2.4176e-01,  1.7212e-01, -1.2596e-01, -3.4117e-01, -1.4624e-01,\n         1.6044e-01, -4.3917e-01,  1.7720e-01, -2.8015e-01, -8.2700e-01,\n         1.8147e-01, -5.9252e-01, -1.0891e+00,  2.2990e-01,  6.1827e-02,\n        -3.5834e-01, -8.8102e-02,  6.1495e-01, -4.5327e-01,  1.7493e-01,\n        -2.1783e-01, -2.5230e-01, -1.6075e-01, -5.8505e-01,  1.7546e-01,\n        -5.5324e-01, -2.4406e-01,  8.8022e-01,  1.9179e-01,  1.0615e-01,\n        -2.6739e-01,  1.0881e-01,  5.8662e-01,  2.3452e-01, -4.4326e-02,\n         6.1231e-01, -9.8901e-02,  5.0935e-01,  4.5667e-01, -2.9151e-01,\n         6.2534e-01, -8.8926e-03,  2.9987e-02, -5.8359e-02, -2.6467e-01,\n         7.0672e-02,  1.0184e+00, -8.8021e-02,  1.0815e-01,  3.0718e-02,\n        -1.8805e-01,  2.6700e-01, -2.6066e-02, -2.5134e-01, -1.9432e-01,\n        -7.7393e-02, -1.8113e-01, -3.7728e-01, -9.1980e-02, -9.0390e-01,\n         4.0578e-01, -2.8211e-01,  7.3154e-02, -8.2849e-02,  2.1628e-01,\n        -2.3450e-01,  5.3261e-01, -3.3509e-01, -2.5267e-01,  3.4817e-01,\n        -6.9541e-01,  3.3209e-02, -3.2857e-01,  7.5939e-03,  4.8555e-01,\n        -2.8322e-01, -3.1377e-01,  1.0620e+00,  2.8980e-01, -6.3535e-01,\n        -4.2767e-01,  2.2828e-01,  2.0297e-01,  1.6051e-01, -1.0083e-01,\n        -6.9135e-01, -9.7826e-01,  4.4057e-02, -6.2607e-01, -2.8473e-01,\n        -3.0164e-01,  5.9125e-01,  2.7972e-01,  6.6367e-01,  4.5764e-01,\n        -1.4154e-01, -2.4290e-01,  3.3742e-01, -6.2466e-02, -3.4298e-02,\n        -3.1275e-01, -5.4819e-01, -5.9546e-01, -6.7419e-01,  2.9088e-01,\n        -1.3515e-01, -2.5165e-01,  3.5038e-01,  4.1604e-01,  6.9316e-01,\n        -4.1241e-01,  3.4430e-01, -1.6449e-01,  5.0773e-01, -1.3581e-01,\n        -2.1827e-01, -4.5381e-02, -3.8861e-01, -5.7996e-01, -1.1078e-01,\n        -2.6628e-01,  2.8737e-01, -1.0107e-01,  1.2837e-01,  2.0963e-01,\n        -3.3976e-01,  3.6092e-02,  2.9142e-01, -4.6523e-01, -3.1954e-01,\n         1.7907e-01,  1.2368e-01, -9.0304e-02,  1.9813e-01, -3.1041e-01,\n         1.0728e+00,  3.4741e-01,  8.1783e-02,  8.9921e-02,  3.6230e-01,\n        -3.0307e-01, -1.9857e-01,  1.9611e-01, -4.3637e-01, -3.4780e-01,\n        -6.7837e-02, -4.4186e-01, -4.7251e-01,  2.1737e-01, -5.7428e-01,\n        -3.0517e-01, -7.3247e-01,  5.9134e-01, -3.4345e-01,  4.9894e-01,\n         3.2862e-01, -3.0654e-01, -4.0629e-01,  1.5006e-01,  2.5368e-01,\n        -2.3007e-01,  1.8403e-03,  7.9848e-01, -1.7357e-01, -4.0463e-01,\n        -5.3868e-01, -2.6530e-01,  2.6512e-01, -2.2232e-03, -3.8730e-01,\n         4.6562e-01, -4.5014e-01, -5.3627e-01, -2.7906e-01,  3.2162e-01,\n         2.1524e-01,  2.2613e-01,  3.9513e-02, -1.2572e-01, -1.0437e-01,\n         6.3167e-01, -1.0575e-01, -8.3391e-02, -6.0210e-01,  9.9847e-01,\n         3.0963e-01, -2.1670e-01, -4.9201e-02, -2.1053e-02,  5.1418e-01,\n         2.7448e-02, -8.2667e-02, -9.0793e-01, -2.1018e-01, -1.5058e-01,\n        -2.2326e-01, -3.1810e-01,  8.4497e-02,  8.8085e-02,  5.0583e-01,\n         1.7896e-01,  1.3755e-01, -4.1343e-02,  3.1611e-01,  3.6648e-01,\n        -1.9640e-01,  2.2066e-01,  2.3228e-03,  2.4874e-01,  1.3535e-01,\n        -4.0606e-02,  1.1332e-01, -5.0368e-01, -2.3094e-01,  3.9733e-03,\n         1.6450e-01, -4.0673e-01, -5.7646e-02, -2.4760e-01,  3.1167e-01,\n         2.1048e-02, -2.4944e-01, -3.5752e-01, -1.8347e-01,  4.9885e-02,\n        -1.6075e-01, -1.2182e+00,  6.6527e-01,  5.6264e-01, -2.1351e-03,\n        -1.0923e-01, -8.8793e-02, -3.1410e-01, -2.1447e-01, -3.6825e-02,\n         4.0950e-01,  5.8010e-01,  4.5523e-01, -1.5202e-01, -1.9784e-01,\n        -2.7890e-01, -7.2032e-01,  3.2933e-02, -9.3587e-01,  4.7786e-01,\n        -8.2722e-02,  5.1388e-01, -1.4285e-01,  2.3218e-01,  1.4397e-01,\n        -3.0674e-01,  5.7743e-01,  8.9223e-02,  1.6272e-01, -3.9965e-01,\n         1.1764e-01,  8.5152e-01,  6.1573e-02, -1.6767e-01, -4.5340e-01,\n         2.0271e-01, -3.3373e-03,  4.0321e-02, -3.6859e-01, -3.2290e-01,\n         5.9043e-01, -2.5570e-01,  3.0624e-02,  4.0597e-01,  6.5783e-01,\n         5.2406e-01, -2.7641e-01,  2.3182e-01,  2.8475e-01,  3.1032e-01,\n         7.4762e-02, -1.5243e-01,  4.2503e-01, -3.8793e-01, -5.3035e-01,\n        -5.9796e-01, -1.3798e-01,  1.6801e-01,  3.9010e-01,  4.9668e-01,\n        -6.4834e-01,  1.9638e-01,  4.2070e-01, -1.0363e+00, -6.0143e-01,\n         1.6587e-01,  3.3468e-02,  4.9227e-01, -4.8176e-01,  5.2882e-01,\n         3.5856e-01,  2.3992e-01,  2.3579e-01, -1.3872e-02,  2.2566e-01,\n        -8.0686e-01, -1.0110e-01,  3.8075e-01,  7.9470e-01, -4.7949e-02,\n        -9.1144e-02,  3.9124e-01,  9.7927e-02,  3.9253e-02,  5.8997e-01,\n         9.9825e-02,  6.3516e-01, -2.0331e-02, -4.7421e-01, -2.3255e-02,\n         1.8895e-01,  1.5852e-01, -1.0881e-01,  2.9127e-01, -1.8899e-01,\n         5.4588e-02,  8.1847e-02,  8.7546e-01,  8.3765e-02, -6.6410e-01,\n        -5.1483e-02, -2.9061e-02,  5.8324e-01,  3.7943e-01, -4.4385e-01,\n         2.0068e-01, -2.3234e-02,  2.5206e-01, -5.4227e-01, -3.6244e-01,\n        -3.6121e-01,  8.8293e-02, -4.6867e-02,  3.0898e-01,  1.2562e-01,\n         1.3403e-01, -1.3644e-01, -2.5941e-01,  4.8034e-02, -1.3953e-01,\n         1.3284e-01, -2.4123e-01, -1.3484e-01,  2.6368e-01, -6.1692e-01,\n         3.3115e-01, -2.0574e-02,  3.9207e-01,  5.1880e-01, -1.9379e-01,\n         1.3109e-01,  1.2283e-01,  7.5041e-01,  2.2792e-01, -4.2666e-02,\n        -2.6896e-01,  3.9793e-01,  4.3664e-01,  6.4404e-01,  2.7733e-01,\n         2.0432e-02, -1.6217e-01, -1.0940e+00,  8.6033e-01,  2.1651e-01,\n         9.7056e-01,  6.1899e-01, -3.3258e-01,  4.4195e-01,  7.4523e-01,\n        -3.7879e-01, -1.5135e-01,  2.1638e-01,  2.1130e-01,  2.1147e-01,\n         3.8634e-01, -3.3077e-01,  1.5987e-01, -3.6976e-01, -2.5714e-01,\n        -7.6617e-01,  3.6630e-01, -1.8148e-01,  2.8788e-01, -3.8311e-01,\n         4.2833e-01, -2.4194e-01, -4.0960e-01, -6.0599e-01,  1.6543e-01,\n         6.9024e-01,  2.9065e-01, -4.0624e-01, -1.9708e-01,  2.0775e-01,\n        -1.5129e-01, -2.9079e-01, -1.8171e-01, -5.0079e-01,  2.1213e-02,\n         4.9969e-01,  3.7623e-01, -1.5275e-01, -1.0508e-01, -4.8076e-01,\n        -4.0909e-01,  1.0569e-01, -5.1499e-03, -3.0281e-01,  2.7713e-01,\n        -1.5791e-01,  3.7641e-01, -2.6180e-02,  1.6192e+00, -4.0088e-01,\n         3.4150e-01,  1.7272e-01, -3.7658e-01, -3.9620e-01, -8.4894e-02,\n         8.6585e-02, -4.7175e-01,  5.8756e-01, -1.5664e-01, -4.8346e-01,\n         1.0611e-01, -1.2376e+00,  1.1992e-01, -7.9114e-04, -2.6662e-01,\n         2.6651e-01,  4.2901e-01, -2.7529e-01, -4.4504e-01, -6.8601e-01,\n         3.1387e-02,  1.1083e-01,  1.3790e-01,  2.1008e-01,  3.9260e-01,\n         4.5936e-02,  4.9995e-01, -8.8466e-02,  7.6394e-01, -5.6177e-02,\n        -1.1638e-01,  8.7215e-01,  4.1881e-01,  7.7081e-01,  1.1811e-01,\n        -2.6469e-02, -4.9407e-01, -1.0964e-01, -2.1156e-02,  2.4196e-01,\n         3.1089e-02, -1.3401e-01, -1.3042e-01,  7.9084e-01, -3.0689e-01,\n        -4.4046e-01, -6.8240e-02, -3.6463e-01, -4.3389e-01,  7.5479e-01,\n         5.3594e-01,  1.4068e-01,  2.4571e-01, -4.2344e-01,  2.9512e-01,\n         5.6413e-01, -4.5374e-01, -6.3161e-01, -7.7501e-01, -7.8680e-01,\n        -3.0878e-03, -1.9007e-01, -7.1114e-01, -3.7428e-01,  7.7047e-02,\n        -2.6771e-01, -1.9949e-01,  2.0194e-01,  6.9938e-01,  6.5487e-01,\n        -3.7823e-01,  5.7787e-02,  7.0958e-01,  3.2488e-01, -2.3790e-01,\n        -2.9901e-01, -2.2386e-01, -6.3187e-01, -2.9926e-01,  2.6260e-01,\n        -5.1997e-01, -4.0482e-01,  1.9843e-01,  1.7424e-02,  6.6369e-01,\n        -1.5160e-02, -1.5641e-01,  2.6553e-01, -7.8634e-02, -7.0266e-03,\n         6.8289e-01, -8.5076e-01,  5.5231e-01,  6.7504e-01,  2.8372e-01,\n         4.1663e-01,  4.4135e-01, -2.7150e-01, -4.4023e-01,  4.5150e-02,\n        -2.8038e-01, -3.8793e-01,  2.7345e-01,  4.7777e-01, -1.4767e-01,\n        -2.6118e-01, -4.7183e-01, -1.4329e-01, -7.6049e-01,  6.7107e-01,\n        -2.0443e-01, -2.7047e-01,  1.1075e-02, -1.2466e+00, -5.9261e-02,\n        -3.8813e-01, -5.0112e-01,  2.6134e-01, -3.9652e-02, -2.5991e-01,\n         1.7323e-01,  1.7811e-01,  3.2428e-01, -2.0230e-01, -2.2913e-01,\n        -5.6674e-01,  2.9269e-01, -1.6355e-01,  5.7632e-02,  2.9661e-01,\n         4.9825e-01, -3.5870e-01, -3.4529e-01, -1.0416e-01,  7.0672e-01,\n         1.2591e+01, -2.8504e-01, -2.4981e-03,  5.5791e-01, -1.0381e-02,\n         4.1563e-01, -6.9304e-02, -2.2553e-02, -4.3496e-01, -6.2645e-02,\n        -1.4835e-01, -7.9678e-01,  5.8739e-02,  9.9835e-02, -1.1845e-01,\n        -4.7807e-01, -2.6397e-02, -6.9972e-02, -1.9753e-02,  2.8168e-01,\n        -2.2004e-01, -5.3054e-01, -5.7222e-01,  2.5014e-01,  3.9314e-01,\n        -4.8859e-03, -2.8143e-01,  1.7481e-01, -3.0121e-01,  7.4098e-01,\n        -9.8594e-01, -3.7403e-02, -5.0580e-01,  8.8005e-03, -4.1146e-02,\n         5.5415e-02,  5.2117e-02,  4.1045e-01, -3.1257e-01, -1.1149e-01,\n         2.6684e-01,  4.3306e-02,  2.3276e-01,  2.9679e-01,  8.1352e-03,\n         5.3841e-01, -3.2758e-01,  1.4128e-01, -7.5950e-01,  4.8830e-01,\n        -1.1367e-02, -2.0504e-01, -2.3682e-01,  5.6255e-01, -8.6762e-02,\n        -4.4402e-01,  4.3863e-01,  4.4898e-02,  1.7972e-02, -1.3639e+00,\n         1.2385e-01,  1.0029e-03,  1.7716e-01, -8.3262e-01, -3.2307e-01,\n        -1.5772e-01, -1.3139e-01, -6.7924e-01, -2.1163e-01, -2.2229e-01,\n        -6.4361e-01,  4.5839e-01, -4.2562e-01,  6.5499e-01,  8.2408e-01,\n         4.5284e-01, -3.2402e-01,  2.0530e-01,  8.2163e-02,  1.6623e-02,\n        -5.5131e-01,  3.7715e-01,  1.7274e-01, -2.4658e-01,  4.1379e-02,\n         1.0836e-01,  5.7532e-02,  5.2291e-02, -1.1903e-01,  4.3320e-01,\n         2.8047e-01, -4.3757e-01, -5.9911e-01, -7.0995e-02, -3.8048e-01,\n         9.0133e-02,  5.9105e-01, -4.1014e-01, -2.9260e-01,  7.1137e-01,\n        -3.4978e-01,  5.5717e-01, -6.7542e-03, -1.2313e+00,  1.4001e-01,\n         1.0509e+00,  3.9520e-01,  1.3878e-01, -1.7445e-01, -1.1293e-01,\n         4.0407e-01, -4.6872e-01, -3.7828e-01, -2.1071e-01, -3.8892e-01,\n        -8.7110e-02,  4.5361e-01,  2.7767e-01,  3.3383e-01, -8.1067e-02,\n         3.4574e-02,  3.1359e-01, -2.8615e-01,  1.9318e-01, -1.5722e-02,\n        -4.3769e-01,  2.1176e-01, -9.4442e-02, -6.2555e-03, -8.0033e-01,\n        -1.5019e-01, -2.5088e-01, -3.4340e-01])\ntorch.Size([768])\n"
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 7, 3072])\ntorch.Size([1, 7, 3072])\ntensor([-0.1732,  0.0409,  0.4544,  ..., -0.5578, -0.2156, -0.0674])\ntorch.Size([3072])\n"
    }
   ],
   "source": [
    "# get last four layers\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# cast layers to a tuple and concatenate over the last dimension\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# take the mean of the concatenated vector over the token dimension\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_hidden_states.shape)\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}