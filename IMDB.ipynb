{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599813062237",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app, logging\n",
    "import sh\n",
    "import torch \n",
    "import pytorch_lightning as pl\n",
    "import nlp\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "lr = 1e-2\n",
    "momentum = .9\n",
    "model = 'bert-base-uncased'\n",
    "seq_length = 32\n",
    "percent = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBSentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = transformers.BertForSequenceClassification.from_pretrained(model)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        tokenizer = transformers.BertTokenizer.from_pretrained(model)\n",
    "\n",
    "        def _tokenize(x):\n",
    "            x['input_ids'] = tokenizer.batch_encode_plus(\n",
    "                    x['text'], \n",
    "                    max_length=seq_length, \n",
    "                    pad_to_max_length=True)['input_ids']\n",
    "            return x\n",
    "\n",
    "        def _prepare_ds(split):\n",
    "            ds = nlp.load_dataset('imdb', split=f'{split}[:{batch_size if debug else f\"{percent}%\"}]')\n",
    "            ds = ds.map(_tokenize, batched=True)\n",
    "            ds.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "            return ds\n",
    "\n",
    "        self.train_ds, self.test_ds = map(_prepare_ds, ('train', 'test'))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        mask = (input_ids != 0).float()\n",
    "        logits, = self.model(input_ids, mask)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(batch['input_ids'])\n",
    "        loss = self.loss(logits, batch['label']).mean()\n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(batch['input_ids'])\n",
    "        loss = self.loss(logits, batch['label'])\n",
    "        acc = (logits.argmax(-1) == batch['label']).float()\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.cat([o['loss'] for o in outputs], 0).mean()\n",
    "        acc = torch.cat([o['acc'] for o in outputs], 0).mean()\n",
    "        out = {'val_loss': loss, 'val_acc': acc}\n",
    "        return {**out, 'log': out}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                self.train_ds,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=True,\n",
    "                shuffle=True,\n",
    "                )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                self.test_ds,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=False,\n",
    "                shuffle=True,\n",
    "                )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = IMDBSentimentClassifier()\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir='logs',\n",
    "        gpus=(1 if torch.cuda.is_available() else 0),\n",
    "        max_epochs=epochs,\n",
    "        fast_dev_run=debug,\n",
    "        logger=pl.loggers.TensorBoardLogger('logs/', name='imdb', version=0),\n",
    "    )\n",
    "    trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nGPU available: False, used: False\nI0911 10:48:01.371520 4590095808 distributed.py:41] GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nI0911 10:48:01.373102 4590095808 distributed.py:41] TPU available: False, using: 0 TPU cores\nI0911 10:48:03.628378 4590095808 load.py:160] Checking /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py for additional imports.\nI0911 10:48:03.629971 4590095808 filelock.py:274] Lock 140690644259936 acquired on /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\nI0911 10:48:03.630580 4590095808 load.py:331] Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb\nI0911 10:48:03.631119 4590095808 load.py:344] Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:03.631819 4590095808 load.py:357] Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py to /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.py\nI0911 10:48:03.632450 4590095808 load.py:371] Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/dataset_infos.json to /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/dataset_infos.json\nI0911 10:48:03.633114 4590095808 load.py:382] Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.json\nI0911 10:48:03.633575 4590095808 filelock.py:318] Lock 140690644259936 released on /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\nI0911 10:48:03.634183 4590095808 builder.py:210] No config specified, defaulting to first: imdb/plain_text\nI0911 10:48:03.634714 4590095808 info.py:236] Loading Dataset Infos from /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:03.635704 4590095808 builder.py:169] Overwrite dataset info from restored data version.\nI0911 10:48:03.636193 4590095808 info.py:194] Loading Dataset info from /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:03.637109 4590095808 builder.py:388] Reusing dataset imdb (/Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743)\nI0911 10:48:03.637898 4590095808 builder.py:589] Constructing Dataset for split train[:5%], from /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:03.642411 4590095808 info_utils.py:39] All the checksums matched successfully for post processing resources\nTruncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nI0911 10:48:03.926768 4590095808 arrow_dataset.py:905] Loading cached processed dataset at /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/cache-5096c9488e5e9121f0e27ef2823aad4b.arrow\nI0911 10:48:03.928406 4590095808 arrow_dataset.py:558] Set __getitem__(key) output type to torch for ['input_ids', 'label'] columns  (when key is int or slice) and don't output other (un-formated) columns.\nI0911 10:48:05.374840 4590095808 load.py:160] Checking /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py for additional imports.\nI0911 10:48:05.376241 4590095808 filelock.py:274] Lock 140690643411968 acquired on /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\nI0911 10:48:05.376975 4590095808 load.py:331] Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb\nI0911 10:48:05.377518 4590095808 load.py:344] Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:05.378479 4590095808 load.py:357] Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py to /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.py\nI0911 10:48:05.379076 4590095808 load.py:371] Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/dataset_infos.json to /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/dataset_infos.json\nI0911 10:48:05.379938 4590095808 load.py:382] Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.json\nI0911 10:48:05.380448 4590095808 filelock.py:318] Lock 140690643411968 released on /Users/janbraunsdorff/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\nI0911 10:48:05.381283 4590095808 builder.py:210] No config specified, defaulting to first: imdb/plain_text\nI0911 10:48:05.381809 4590095808 info.py:236] Loading Dataset Infos from /Users/janbraunsdorff/opt/anaconda3/lib/python3.8/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:05.383157 4590095808 builder.py:169] Overwrite dataset info from restored data version.\nI0911 10:48:05.383641 4590095808 info.py:194] Loading Dataset info from /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:05.384372 4590095808 builder.py:388] Reusing dataset imdb (/Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743)\nI0911 10:48:05.384951 4590095808 builder.py:589] Constructing Dataset for split test[:5%], from /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\nI0911 10:48:05.388358 4590095808 info_utils.py:39] All the checksums matched successfully for post processing resources\nTruncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nI0911 10:48:05.644897 4590095808 arrow_dataset.py:905] Loading cached processed dataset at /Users/janbraunsdorff/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/cache-29e71fa4b8c2e1308adee2a9ef1bf64e.arrow\nI0911 10:48:05.646729 4590095808 arrow_dataset.py:558] Set __getitem__(key) output type to torch for ['input_ids', 'label'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n\n  | Name  | Type                          | Params\n--------------------------------------------------------\n0 | model | BertForSequenceClassification | 109 M \n1 | loss  | CrossEntropyLoss              | 0     \nI0911 10:48:05.673185 4590095808 lightning.py:1449] \n  | Name  | Type                          | Params\n--------------------------------------------------------\n0 | model | BertForSequenceClassification | 109 M \n1 | loss  | CrossEntropyLoss              | 0     \nEpoch 0:  50%|████▉     | 156/313 [02:59<03:00,  1.15s/it, loss=0.000, v_num=0]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 0:  50%|█████     | 157/313 [02:59<02:58,  1.14s/it, loss=0.000, v_num=0]\nEpoch 0:  50%|█████     | 158/313 [02:59<02:56,  1.14s/it, loss=0.000, v_num=0]\nEpoch 0:  51%|█████     | 159/313 [03:00<02:54,  1.13s/it, loss=0.000, v_num=0]\nEpoch 0:  51%|█████     | 160/313 [03:00<02:52,  1.13s/it, loss=0.000, v_num=0]\nEpoch 0:  51%|█████▏    | 161/313 [03:00<02:50,  1.12s/it, loss=0.000, v_num=0]\nEpoch 0:  52%|█████▏    | 162/313 [03:00<02:48,  1.12s/it, loss=0.000, v_num=0]\nEpoch 0:  52%|█████▏    | 163/313 [03:01<02:46,  1.11s/it, loss=0.000, v_num=0]\nEpoch 0:  52%|█████▏    | 164/313 [03:01<02:44,  1.11s/it, loss=0.000, v_num=0]\nEpoch 0:  53%|█████▎    | 165/313 [03:01<02:43,  1.10s/it, loss=0.000, v_num=0]\nEpoch 0:  53%|█████▎    | 166/313 [03:02<02:41,  1.10s/it, loss=0.000, v_num=0]\nEpoch 0:  53%|█████▎    | 167/313 [03:02<02:39,  1.09s/it, loss=0.000, v_num=0]\nEpoch 0:  54%|█████▎    | 168/313 [03:02<02:37,  1.09s/it, loss=0.000, v_num=0]\nEpoch 0:  54%|█████▍    | 169/313 [03:02<02:35,  1.08s/it, loss=0.000, v_num=0]\nEpoch 0:  54%|█████▍    | 170/313 [03:03<02:34,  1.08s/it, loss=0.000, v_num=0]\nEpoch 0:  55%|█████▍    | 171/313 [03:03<02:32,  1.07s/it, loss=0.000, v_num=0]\nEpoch 0:  55%|█████▍    | 172/313 [03:03<02:30,  1.07s/it, loss=0.000, v_num=0]\nEpoch 0:  55%|█████▌    | 173/313 [03:03<02:28,  1.06s/it, loss=0.000, v_num=0]\nEpoch 0:  56%|█████▌    | 174/313 [03:04<02:27,  1.06s/it, loss=0.000, v_num=0]\nEpoch 0:  56%|█████▌    | 175/313 [03:04<02:25,  1.05s/it, loss=0.000, v_num=0]\nEpoch 0:  56%|█████▌    | 176/313 [03:04<02:23,  1.05s/it, loss=0.000, v_num=0]\nEpoch 0:  57%|█████▋    | 177/313 [03:04<02:21,  1.04s/it, loss=0.000, v_num=0]\nEpoch 0:  57%|█████▋    | 178/313 [03:05<02:20,  1.04s/it, loss=0.000, v_num=0]\nEpoch 0:  57%|█████▋    | 179/313 [03:05<02:18,  1.04s/it, loss=0.000, v_num=0]\nEpoch 0:  58%|█████▊    | 180/313 [03:05<02:17,  1.03s/it, loss=0.000, v_num=0]\nEpoch 0:  58%|█████▊    | 181/313 [03:05<02:15,  1.03s/it, loss=0.000, v_num=0]\nEpoch 0:  58%|█████▊    | 182/313 [03:05<02:13,  1.02s/it, loss=0.000, v_num=0]\nEpoch 0:  58%|█████▊    | 183/313 [03:06<02:12,  1.02s/it, loss=0.000, v_num=0]\nEpoch 0:  59%|█████▉    | 184/313 [03:06<02:10,  1.01s/it, loss=0.000, v_num=0]\nEpoch 0:  59%|█████▉    | 185/313 [03:06<02:09,  1.01s/it, loss=0.000, v_num=0]\nEpoch 0:  59%|█████▉    | 186/313 [03:06<02:07,  1.00s/it, loss=0.000, v_num=0]\nEpoch 0:  60%|█████▉    | 187/313 [03:07<02:06,  1.00s/it, loss=0.000, v_num=0]\nEpoch 0:  60%|██████    | 188/313 [03:07<02:04,  1.00it/s, loss=0.000, v_num=0]\nEpoch 0:  60%|██████    | 189/313 [03:07<02:03,  1.01it/s, loss=0.000, v_num=0]\nEpoch 0:  61%|██████    | 190/313 [03:07<02:01,  1.01it/s, loss=0.000, v_num=0]\nEpoch 0:  61%|██████    | 191/313 [03:08<02:00,  1.02it/s, loss=0.000, v_num=0]\nEpoch 0:  61%|██████▏   | 192/313 [03:08<01:58,  1.02it/s, loss=0.000, v_num=0]\nEpoch 0:  62%|██████▏   | 193/313 [03:08<01:57,  1.02it/s, loss=0.000, v_num=0]\nEpoch 0:  62%|██████▏   | 194/313 [03:08<01:55,  1.03it/s, loss=0.000, v_num=0]\nEpoch 0:  62%|██████▏   | 195/313 [03:09<01:54,  1.03it/s, loss=0.000, v_num=0]\nEpoch 0:  63%|██████▎   | 196/313 [03:09<01:53,  1.04it/s, loss=0.000, v_num=0]\nEpoch 0:  63%|██████▎   | 197/313 [03:09<01:51,  1.04it/s, loss=0.000, v_num=0]\nEpoch 0:  63%|██████▎   | 198/313 [03:09<01:50,  1.04it/s, loss=0.000, v_num=0]\nEpoch 0:  64%|██████▎   | 199/313 [03:10<01:48,  1.05it/s, loss=0.000, v_num=0]\nEpoch 0:  64%|██████▍   | 200/313 [03:10<01:47,  1.05it/s, loss=0.000, v_num=0]\nEpoch 0:  64%|██████▍   | 201/313 [03:10<01:46,  1.06it/s, loss=0.000, v_num=0]\nEpoch 0:  65%|██████▍   | 202/313 [03:10<01:44,  1.06it/s, loss=0.000, v_num=0]\nEpoch 0:  65%|██████▍   | 203/313 [03:10<01:43,  1.06it/s, loss=0.000, v_num=0]\nEpoch 0:  65%|██████▌   | 204/313 [03:11<01:42,  1.07it/s, loss=0.000, v_num=0]\nEpoch 0:  65%|██████▌   | 205/313 [03:11<01:40,  1.07it/s, loss=0.000, v_num=0]\nEpoch 0:  66%|██████▌   | 206/313 [03:11<01:39,  1.07it/s, loss=0.000, v_num=0]\nEpoch 0:  66%|██████▌   | 207/313 [03:11<01:38,  1.08it/s, loss=0.000, v_num=0]\nEpoch 0:  66%|██████▋   | 208/313 [03:12<01:37,  1.08it/s, loss=0.000, v_num=0]Saving latest checkpoint..\nI0911 10:51:18.364202 4590095808 training_loop.py:1136] Saving latest checkpoint..\nEpoch 0:  66%|██████▋   | 208/313 [03:12<01:37,  1.08it/s, loss=0.000, v_num=0]\n"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}