{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Weil Bananen böse sind und sie krumme Sachen machen\n",
    "Weil sie Leute um die Ecke bringen\n",
    "Weil ihr gerade Sachen viel zu Mainstream waren\n",
    "Weil der Erderwärmung\n",
    "Weil die Iluminaten das so wollen\n",
    "Weil sie Langeweile hatte\n",
    "Weil wegen halt ist so\n",
    "Weil sie wenn sie gerade wär ja keine Banane wär\n",
    "Weil sie wegen übermäßiger Masturbation keine Erektion mehr verspürt\n",
    "Weil sie Jahre lang einen Bogen um die DDR machte\n",
    "Weil einer auf dem Acker wetzte und sich auf die Banane setzte\n",
    "Weil die Affen dran gefummelt haben\n",
    "Es gab Bemühungen in der DDR Bananen mittels kubanischer Spreewaldgurken nachzubauen aus Verzweiflung haben sich die Bananen deshalb krumm gemacht\n",
    "Weil die Affen im Urwald drauf schaukeln tun\n",
    "Weil sie seit Jahrtausenden im Urwald Limbo tanzen dadurch ist der Rücken der Bananen ziemlich krumm geworden\n",
    "Weil sie sonst nicht in die Schale passt\n",
    "Weil sie durch die Bananenbatterie so geformt wurde\n",
    "Das kommt vom Mond\n",
    "Auch Bananen haben das Bedürfnis ihren unteren Teil zu begutachten  Ein nach vorne beugen ist dabei zwingend\n",
    "Eine andere Theorie besagt dass sie sich aufgrund der Erdanziehung im Laufe der Zeit krümmen\n",
    "Bananen sind seit dem Bananenkrieg von krumm\n",
    "Bananen sind krumm weil die Raum Zeit sich biegt und die Bananen biegen sich mit\n",
    "Weil sie von der Sonne gekitzelt wurde\n",
    "Weil sie zu dumm war sich gerade zu biegen\n",
    "Weil sie beim Sit Up machen erstarrt sind\n",
    "Weil sie zu viel Unsinn angestellt haben\n",
    "Weil sie verstecken gespielt hat mit anderen Bananen und und hat dabei versucht sich kleine zu machen was leider nicht ganz klappte\n",
    "Weil niemand in den Urwald zog und die Banane gerade bog\n",
    "Weil Baum\n",
    "Weil sie sich krumm gelacht haben\n",
    "Weil man sie dann so viel besser als Spielzeugpistole verwenden kann\n",
    "Damit sie leichter an das männliche Glied erinnert\n",
    "Weil sie gelb sind\n",
    "Albert Einstein sagte Große Massen krümmen den Raum Die Erde hat eine große Masse und deshalb krümmt sie den Raum um die Banane herum Die Krummheit der Banane ist dieser Theorie also nur eine Illusion denn innerhalb des gekrümmten Raums ist die Banane natürlich völlig gerade oder so\n",
    "Weil sie als KanonenFutter benuzt wurde\n",
    "Weil man sie nicht nur zum Essen verwendet hat\n",
    "Weil sie es kann\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(['weil', 'böse'], 'bananen'), (['bananen', 'sind'], 'böse'), (['böse', 'und'], 'sind')]\n"
    }
   ],
   "source": [
    "sencentce = text.lower().split()\n",
    "trigrams = [([sencentce[i-1], sencentce[i +1]], sencentce[i]) for i in range(1, len(sencentce) - 1)]\n",
    "print(trigrams[:3])\n",
    "vocab = list(set(sencentce))\n",
    "vocab.append('-')\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "class NGramLanguage(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_sice, emb_size, context_size):\n",
    "        super(NGramLanguage, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sice, emb_size)\n",
    "        self.linear1 = nn.Linear(context_size * emb_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_sice)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x).view((1, -1))\n",
    "        out = F.relu(self.linear1(emb))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguage(len(vocab), 10, 2)\n",
    "op = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 368/368 [00:00<00:00, 392.46it/s, loss=6.06]\n100%|██████████| 368/368 [00:00<00:00, 398.66it/s, loss=5.7]\n100%|██████████| 368/368 [00:00<00:00, 395.53it/s, loss=5.36]\n100%|██████████| 368/368 [00:00<00:00, 371.57it/s, loss=5.01]\n100%|██████████| 368/368 [00:00<00:00, 382.05it/s, loss=4.63]\n100%|██████████| 368/368 [00:00<00:00, 383.97it/s, loss=4.22]\n100%|██████████| 368/368 [00:01<00:00, 359.28it/s, loss=3.81]\n100%|██████████| 368/368 [00:00<00:00, 389.21it/s, loss=3.43]\n100%|██████████| 368/368 [00:01<00:00, 335.93it/s, loss=3.11]\n100%|██████████| 368/368 [00:01<00:00, 347.30it/s, loss=2.86]\n100%|██████████| 368/368 [00:00<00:00, 374.01it/s, loss=2.65]\n100%|██████████| 368/368 [00:01<00:00, 363.68it/s, loss=2.46]\n100%|██████████| 368/368 [00:01<00:00, 363.01it/s, loss=2.27]\n100%|██████████| 368/368 [00:01<00:00, 344.93it/s, loss=2.08]\n100%|██████████| 368/368 [00:01<00:00, 334.28it/s, loss=1.87]\n100%|██████████| 368/368 [00:01<00:00, 343.36it/s, loss=1.67]\n100%|██████████| 368/368 [00:00<00:00, 391.13it/s, loss=1.47]\n100%|██████████| 368/368 [00:00<00:00, 385.99it/s, loss=1.27]\n100%|██████████| 368/368 [00:01<00:00, 354.97it/s, loss=1.09]\n100%|██████████| 368/368 [00:01<00:00, 360.69it/s, loss=0.925]\n100%|██████████| 368/368 [00:01<00:00, 359.10it/s, loss=0.788]\n100%|██████████| 368/368 [00:00<00:00, 398.02it/s, loss=0.67]\n100%|██████████| 368/368 [00:00<00:00, 370.27it/s, loss=0.58]\n100%|██████████| 368/368 [00:01<00:00, 339.17it/s, loss=0.501]\n100%|██████████| 368/368 [00:00<00:00, 370.42it/s, loss=0.439]\n"
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    iterator = tqdm.tqdm(trigrams)\n",
    "    for context, target in iterator:\n",
    "        context_ids = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_ids)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "        total_loss += loss.item()\n",
    "        iterator.set_postfix(loss=float(loss))\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NGramLanguage(\n  (embedding): Embedding(191, 10)\n  (linear1): Linear(in_features=20, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=191, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ -2.9355,  -6.8245,  -8.3649, -10.4258, -10.4433,  -9.9690,  -8.4168,\n         -12.4639,  -8.7139,  -7.2109,  -7.3466,  -7.8919,  -7.3603,  -8.5415,\n          -5.4777,  -8.3989,  -8.8717,  -8.0988,  -5.5351,  -5.1989,  -8.3598,\n          -9.5222, -11.0208,  -4.3097,  -7.3706, -10.1297,  -9.1615, -10.3087,\n          -7.8628,  -9.7942,  -7.1953,  -9.3680,  -4.1939, -10.9322,  -6.1546,\n          -4.8654, -10.3082,  -6.7962,  -9.8922,  -7.6840, -10.1978,  -7.9522,\n          -7.1923,  -7.7577,  -8.2595,  -7.5430,  -7.4196,  -9.0294,  -5.7491,\n          -8.1743,  -8.1038,  -9.2269, -10.6906, -10.2761,  -6.9589, -10.1868,\n          -6.9804,  -9.1068,  -7.2283,  -4.2672,  -8.7693,  -9.2917,  -4.6886,\n          -7.6183,  -1.9595,  -6.8424,  -8.6870,  -8.5467,  -2.3191,  -8.2937,\n          -8.1097,  -8.7379,  -9.5606,  -6.7030,  -7.9353,  -9.9872,  -2.6664,\n          -6.9691, -11.4396,  -6.9002,  -7.6009,  -8.9282,  -9.4039,  -7.0506,\n          -5.6116,  -7.4902,  -5.4555,  -9.4438,  -8.1928,  -8.9014,  -7.4642,\n         -11.8696,  -5.9368,  -8.1699,  -5.6053,  -8.0993, -10.7341,  -7.1675,\n          -8.3081, -11.5872,  -9.1016,  -9.6790,  -7.2113,  -8.6676, -10.4699,\n         -10.3399,  -6.0755,  -9.8424, -10.5080,  -8.0026,  -9.0391,  -8.2805,\n          -9.4818,  -9.8445, -10.0802,  -6.4507,  -8.7836,  -7.9961,  -4.6736,\n          -8.3182,  -9.7620,  -9.5909,  -6.6340,  -7.2070, -10.6751,  -6.8362,\n          -3.8473,  -8.0948,  -6.7659,  -6.2238,  -8.5672,  -9.3511, -10.5453,\n         -10.2077, -10.6466,  -4.0309, -10.7966,  -7.5161,  -7.4110,  -9.3437,\n         -11.0343,  -3.3867,  -7.4526,  -7.5255,  -8.9679,  -9.0042,  -7.4985,\n          -5.9688,  -6.2718,  -6.5120,  -4.4051,  -9.5724,  -9.7358,  -9.8209,\n          -7.8597,  -6.4475,  -4.1559, -10.7831, -11.4981,  -8.0761, -10.5027,\n          -7.8273,  -8.3382,  -8.7200,  -8.5168,  -8.1796,  -7.1679, -10.9026,\n          -8.7251,  -3.4389,  -3.2046,  -8.2588,  -5.8521,  -9.6990,  -1.2229,\n         -11.1696,  -5.2252,  -8.5985,  -9.1090,  -7.9708, -11.5470,  -9.3550,\n         -10.1775,  -7.2814,  -5.7658,  -9.1863,  -9.9304, -11.3002,  -9.9341,\n         -10.2689, -10.1567]])\ndas\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context = ['-', 'männliche']\n",
    "    context_ids = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    pred = model(context_ids)\n",
    "    print(pred)\n",
    "    index_of_prediction = np.argmax(pred)\n",
    "    print(vocab[index_of_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVec():\n",
    "    def __init__(self, emb):\n",
    "        super(WordVec, self).__init__()\n",
    "        self.embedding = emb\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x).view((1, -1))\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ -4.4048,  -8.2368, -10.6210, -11.1354, -10.5664, -11.6668, -11.2160,\n         -10.6912,  -9.5073, -11.2875, -11.5766, -10.5343,  -6.6726,  -7.9250,\n         -14.3781, -10.3146, -10.3807,  -9.2874,  -9.4006, -10.3426,  -7.7176,\n         -10.5697, -12.1443,  -7.3634, -13.1275, -11.9956, -12.6232, -10.4637,\n         -13.2394,  -9.1006,  -2.9371, -12.2432, -10.9642,  -8.2179, -11.2230,\n          -9.8347, -11.9225,  -6.2976, -10.6430,  -5.9998, -10.0068,  -8.6215,\n         -10.2026, -11.0080,  -8.8340, -11.2243, -11.4376, -12.4624,  -6.8494,\n          -9.4855,  -9.6710, -15.5552, -12.4407, -11.5040, -10.1961,  -6.5366,\n          -7.9872, -10.8028,  -8.2779,  -9.0159,  -9.6937, -10.4606,  -5.6278,\n          -9.6047,  -7.2292,  -9.1991,  -9.9766,  -6.8501,  -8.1059, -10.2993,\n          -9.2062, -10.5814, -10.4844, -10.9201,  -7.8983, -11.3646,  -0.4324,\n          -8.8614, -13.1491, -10.3902,  -9.5909,  -9.1018,  -9.6424,  -6.8304,\n          -6.6116, -10.2972,  -8.2575, -11.4330,  -9.7705, -13.4127,  -6.4684,\n         -11.1080,  -9.2142, -11.1231,  -6.7369,  -9.1545,  -9.6169,  -9.2291,\n         -10.2644, -11.9314,  -7.1676,  -9.8587, -10.0449, -10.9501, -13.3993,\n         -14.4276, -10.7724, -10.6637, -12.1693, -10.1825, -10.6977, -10.4741,\n         -13.9126, -11.8956, -12.4204,  -5.3570, -11.6772,  -9.9344, -10.0094,\n         -10.7704, -10.6716, -12.6438,  -9.9323, -11.5035, -11.5862,  -7.4813,\n          -1.4445,  -6.5204,  -8.5643,  -6.5026, -11.7389, -10.1265, -10.6070,\n         -11.7298,  -8.2920,  -6.5376,  -8.8912, -10.1892, -14.4431, -12.7619,\n         -12.4707,  -7.9714, -10.2153,  -9.2557, -12.0721, -12.4135, -10.7383,\n         -10.9648,  -8.1257, -10.5398, -13.0428, -11.7286, -11.4688, -10.0125,\n         -11.3812,  -8.9639,  -6.8487, -13.0986, -11.6411,  -8.5885,  -6.6339,\n          -9.2476,  -7.7319, -10.0889, -12.4354, -11.7188,  -7.1276, -10.5954,\n         -12.8730,  -8.3716,  -9.2137,  -8.0862,  -6.1316, -10.8895,  -6.1491,\n          -9.8684,  -7.0744, -11.7616,  -9.0829,  -9.5643, -13.9928, -10.0977,\n          -7.8891, -11.3324,  -7.1659,  -7.1840, -12.8079, -12.0036, -11.3172,\n         -12.1301, -12.3688]])\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    word_vec = WordVec(model.embedding)\n",
    "    context= ['-', 'ddr']\n",
    "    context_ids = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    pred = model(context_ids)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}